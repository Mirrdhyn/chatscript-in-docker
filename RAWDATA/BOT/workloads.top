# Kubernetes Workload Resources - bilingual EN/FR
# Covers: Pod, Deployment, StatefulSet, DaemonSet, ReplicaSet, Job, CronJob

# ===========================================================================
# POD
# ===========================================================================

topic: ~pod (pod pods conteneur conteneurs container containers workload workloads charge)

#! what is a pod
u: (<< [what que "qu est-ce"] [is est] [a un une] pod >>)
	^keep() A Pod is the smallest deployable unit in Kubernetes. It represents one or more containers that share networking and storage, and run on the same node. \n
	Un Pod est la plus petite unite deployable dans Kubernetes. Il represente un ou plusieurs conteneurs qui partagent le reseau et le stockage, et s'executent sur le meme noeud.

#! pod spec containers field
u: (<< pod [spec specification] [containers conteneurs container conteneur] >>)
	^keep() The 'containers' field in a Pod spec is a required array defining the containers to run. Each entry specifies image, name, ports, env, command, args, volumeMounts, resources, and lifecycle hooks. \n
	Le champ 'containers' dans la spec d'un Pod est un tableau obligatoire qui definit les conteneurs a executer. Chaque entree precise image, name, ports, env, command, args, volumeMounts, resources et les hooks de cycle de vie.

#! pod init containers
u: (<< pod [init initialisation] [containers conteneurs container conteneur] >>)
	^keep() initContainers run sequentially before app containers start. They are useful for setup tasks like downloading config files, waiting for a service, or running database migrations. Each must complete successfully before the next starts. \n
	Les initContainers s'executent sequentiellement avant le demarrage des conteneurs applicatifs. Ils sont utiles pour des taches de preparation comme le telechargement de fichiers de configuration, l'attente d'un service ou les migrations de base de donnees.

#! pod volumes
u: (<< pod [volumes volume stockage storage] >>)
	^keep() The 'volumes' field in a Pod spec defines storage volumes available to containers. Types include emptyDir, hostPath, configMap, secret, persistentVolumeClaim, projected, and downwardAPI. Containers reference them via volumeMounts. \n
	Le champ 'volumes' definit les volumes de stockage disponibles pour les conteneurs. Les types incluent emptyDir, hostPath, configMap, secret, persistentVolumeClaim, projected et downwardAPI. Les conteneurs y accedent via volumeMounts.

#! pod restart policy
u: (<< pod [restart redemarrage redemarre] [policy politique strategie] >>)
	^keep() The restartPolicy field applies to all containers in the Pod. Values are: Always (default, used by Deployments), OnFailure (used by Jobs), and Never. Kubernetes uses exponential back-off for restarts (10s, 20s, 40s, capped at 5 minutes). \n
	Le champ restartPolicy s'applique a tous les conteneurs du Pod. Les valeurs sont : Always (par defaut, utilise par les Deployments), OnFailure (utilise par les Jobs) et Never. Kubernetes utilise un back-off exponentiel pour les redemarrages.

#! pod affinity anti-affinity
u: (<< pod [affinity affinite] >>)
	^keep() Pod affinity and anti-affinity let you constrain scheduling based on labels of Pods already running on nodes. Use podAffinity to co-locate related Pods and podAntiAffinity to spread them apart. Both support requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution. \n
	L'affinite et l'anti-affinite de Pod permettent de contraindre le placement en fonction des labels des Pods deja presents sur les noeuds. Utilisez podAffinity pour co-localiser et podAntiAffinity pour separer les Pods.

#! pod tolerations taints
u: (<< pod [toleration tolerations taint taints] >>)
	^keep() Taints are applied to nodes to repel Pods. Tolerations are set on Pods to allow scheduling on tainted nodes. A toleration matches a taint by key, operator (Equal or Exists), value, and effect (NoSchedule, PreferNoSchedule, NoExecute). \n
	Les taints sont appliquees aux noeuds pour repousser les Pods. Les tolerations sont definies sur les Pods pour permettre le placement sur des noeuds tainted. Une toleration correspond a un taint par key, operator, value et effect.

#! pod security context
u: (<< pod [security securite] [context contexte] >>)
	^keep() The Pod-level securityContext sets security attributes for all containers. Key fields include: runAsUser (UID), runAsNonRoot (boolean, prevents root), runAsGroup (GID), fsGroup (volume ownership GID), supplementalGroups, seccompProfile, and sysctls. \n
	Le securityContext au niveau Pod definit les attributs de securite pour tous les conteneurs. Les champs cles sont : runAsUser, runAsNonRoot, runAsGroup, fsGroup, supplementalGroups, seccompProfile et sysctls.

#! pod resource requests limits
u: (<< pod [resource ressource resources ressources] [request requests limite limits] >>)
	^keep() Each container can specify resources.requests (minimum guaranteed) and resources.limits (maximum allowed) for CPU and memory. CPU is measured in millicores (e.g. 500m = 0.5 CPU). Memory uses suffixes like Mi and Gi. Requests affect scheduling; limits enforce cgroups constraints. \n
	Chaque conteneur peut specifier resources.requests (minimum garanti) et resources.limits (maximum autorise) pour le CPU et la memoire. Le CPU se mesure en millicores (ex: 500m = 0.5 CPU). La memoire utilise Mi et Gi. Les requests affectent le placement ; les limits appliquent les contraintes cgroups.

#! pod probes liveness readiness startup
u: (<< pod [probe probes sonde sondes liveness readiness startup] >>)
	^keep() Kubernetes supports three probe types per container: livenessProbe (restarts unhealthy containers), readinessProbe (controls traffic routing via Services), and startupProbe (delays other probes until the app starts). Probes can use httpGet, tcpSocket, exec, or grpc. Configure initialDelaySeconds, periodSeconds, timeoutSeconds, failureThreshold, and successThreshold. \n
	Kubernetes supporte trois types de sonde par conteneur : livenessProbe (redemarre les conteneurs defaillants), readinessProbe (controle le routage du trafic), startupProbe (retarde les autres sondes). Les sondes utilisent httpGet, tcpSocket, exec ou grpc.

#! pod DNS policy config
u: (<< pod [dns] [policy politique config configuration] >>)
	^keep() dnsPolicy controls DNS resolution for a Pod. Values: ClusterFirst (default, uses cluster DNS), Default (inherits node DNS), ClusterFirstWithHostNet (for hostNetwork Pods), and None (requires manual dnsConfig). The dnsConfig field lets you add nameservers, searches, and ndots options. \n
	dnsPolicy controle la resolution DNS d'un Pod. Valeurs : ClusterFirst (par defaut), Default (herite du noeud), ClusterFirstWithHostNet, et None (necessite dnsConfig). Le champ dnsConfig permet d'ajouter nameservers, searches et options ndots.

#! pod service account
u: (<< pod [service] [account compte] >>)
	^keep() The serviceAccountName field specifies which ServiceAccount the Pod uses to authenticate with the Kubernetes API. This controls RBAC permissions. Since Kubernetes 1.24, tokens are projected via the TokenRequest API instead of using static Secrets. Set automountServiceAccountToken to false if API access is not needed. \n
	Le champ serviceAccountName indique quel ServiceAccount le Pod utilise pour s'authentifier aupres de l'API Kubernetes. Cela controle les permissions RBAC. Depuis Kubernetes 1.24, les tokens sont injectes via l'API TokenRequest.

#! pod lifecycle phases
u: (<< pod [lifecycle cycle vie] [phase phases etat etats status statut] >>)
	^keep() A Pod goes through these phases: Pending (accepted but not yet scheduled or pulling images), Running (at least one container is running), Succeeded (all containers terminated with exit code 0), Failed (at least one container failed), and Unknown (node communication lost). \n
	Un Pod passe par ces phases : Pending (accepte mais pas encore planifie), Running (au moins un conteneur tourne), Succeeded (tous les conteneurs termines avec code 0), Failed (au moins un conteneur echoue) et Unknown (communication avec le noeud perdue).

# ===========================================================================
# DEPLOYMENT
# ===========================================================================

topic: ~deployment (deployment deployments deploiement deploiements deploy deployer replica replicas)

#! what is a deployment
u: (<< [what que "qu est-ce"] [is est] [a un une] deployment >>)
	^keep() A Deployment provides declarative updates for Pods and ReplicaSets. It manages the desired state of your application, handles rolling updates, rollbacks, and scaling. It is the most common way to run stateless applications in Kubernetes. \n
	Un Deployment fournit des mises a jour declaratives pour les Pods et ReplicaSets. Il gere l'etat souhaite de votre application, les mises a jour progressives, les rollbacks et la mise a l'echelle. C'est le moyen le plus courant d'executer des applications stateless.

#! deployment spec replicas selector template
u: (<< deployment [spec specification] [replicas selector template] >>)
	^keep() Key Deployment spec fields: 'replicas' sets the desired Pod count (default 1). 'selector' is a label selector that must match the Pod template labels (matchLabels). 'template' defines the Pod specification including metadata.labels and the container spec. The selector is immutable after creation. \n
	Champs cles de la spec Deployment : 'replicas' definit le nombre de Pods souhaite. 'selector' est un selecteur de labels qui doit correspondre aux labels du template. 'template' definit la specification du Pod. Le selector est immuable apres creation.

#! deployment strategy rolling update recreate
u: (<< deployment [strategy strategie] >>)
	^keep() Deployment supports two strategy types: RollingUpdate (default) gradually replaces old Pods with new ones, ensuring availability during updates. Recreate kills all existing Pods before creating new ones, causing downtime but avoiding version coexistence issues. \n
	Le Deployment supporte deux types de strategie : RollingUpdate (par defaut) remplace progressivement les anciens Pods. Recreate tue tous les Pods existants avant d'en creer de nouveaux, causant une interruption mais evitant la coexistence de versions.

#! deployment maxSurge maxUnavailable
u: (<< deployment [maxSurge maxUnavailable surge unavailable disponible indisponible] >>)
	^keep() In a RollingUpdate strategy, maxSurge sets the maximum number of Pods that can be created above the desired count (default 25%). maxUnavailable sets the maximum number of Pods that can be unavailable during the update (default 25%). Both accept absolute numbers or percentages. \n
	Dans une strategie RollingUpdate, maxSurge definit le nombre max de Pods au-dessus du nombre souhaite (defaut 25%). maxUnavailable definit le nombre max de Pods indisponibles pendant la mise a jour (defaut 25%). Les deux acceptent des nombres absolus ou des pourcentages.

#! deployment progressDeadlineSeconds
u: (<< deployment [progress progressDeadlineSeconds deadline delai] >>)
	^keep() progressDeadlineSeconds specifies the number of seconds the Deployment controller waits before reporting that a rollout has stalled (default 600s). If no progress is made within this window, the Deployment condition is set to Progressing=False. This does NOT cause an automatic rollback. \n
	progressDeadlineSeconds specifie le nombre de secondes avant que le controleur signale qu'un deploiement est bloque (defaut 600s). Si aucun progres n'est fait, la condition Progressing=False est definie. Cela ne provoque PAS de rollback automatique.

#! deployment revisionHistoryLimit
u: (<< deployment [revision history historique] [limit limite] >>)
	^keep() revisionHistoryLimit specifies the number of old ReplicaSets to retain for rollback purposes (default 10). Setting it to 0 means you cannot roll back. Old ReplicaSets consume etcd storage, so balance rollback needs with resource usage. \n
	revisionHistoryLimit specifie le nombre d'anciens ReplicaSets a conserver pour les rollbacks (defaut 10). Le mettre a 0 signifie que vous ne pouvez plus revenir en arriere. Les anciens ReplicaSets consomment du stockage etcd.

#! how to rollback a deployment
u: (<< [rollback revenir retour annuler undo] deployment >>)
	^keep() To rollback a Deployment: 'kubectl rollout undo deployment/<name>' rolls back to the previous revision. Use '--to-revision=N' for a specific revision. Check history with 'kubectl rollout history deployment/<name>'. Each rollback creates a new revision entry. \n
	Pour faire un rollback : 'kubectl rollout undo deployment/<name>' revient a la revision precedente. Utilisez '--to-revision=N' pour une revision specifique. Consultez l'historique avec 'kubectl rollout history deployment/<name>'.

#! deployment vs replicaset difference
u: (<< deployment [vs versus contre versus ou] [replicaset "replica set"] >>)
	^keep() A Deployment manages ReplicaSets, which in turn manage Pods. The Deployment adds rolling update capabilities, rollback history, and declarative update strategies on top of ReplicaSet. You should almost always use a Deployment rather than creating ReplicaSets directly. \n
	Un Deployment gere des ReplicaSets, qui a leur tour gerent des Pods. Le Deployment ajoute les mises a jour progressives, l'historique de rollback et les strategies de mise a jour. Vous devriez presque toujours utiliser un Deployment plutot que des ReplicaSets directement.

# ===========================================================================
# STATEFULSET
# ===========================================================================

topic: ~statefulset (statefulset statefulsets stateful etat)

#! what is a statefulset
u: (<< [what que "qu est-ce"] [is est] [a un une] statefulset >>)
	^keep() A StatefulSet manages stateful applications. Unlike Deployments, it provides stable network identities (pod-0, pod-1, ...), stable persistent storage per Pod, and ordered deployment, scaling, and deletion. Use it for databases, distributed systems, and any app needing stable identity. \n
	Un StatefulSet gere les applications avec etat. Contrairement aux Deployments, il fournit des identites reseau stables (pod-0, pod-1, ...), un stockage persistant stable par Pod, et un deploiement ordonne. Utilisez-le pour les bases de donnees et les systemes distribues.

#! statefulset volumeClaimTemplates
u: (<< statefulset [volume volumeClaimTemplates claim persistant persistent] >>)
	^keep() volumeClaimTemplates in a StatefulSet spec automatically create a PersistentVolumeClaim for each Pod. Each PVC gets a unique name based on the template name and Pod ordinal (e.g. data-myapp-0). PVCs persist across Pod restarts and are NOT automatically deleted when the StatefulSet scales down. \n
	volumeClaimTemplates dans la spec d'un StatefulSet creent automatiquement un PVC pour chaque Pod. Chaque PVC recoit un nom unique base sur le nom du template et l'ordinal du Pod. Les PVC persistent au-dela des redemarrages et ne sont PAS supprimes automatiquement lors d'un scale down.

#! statefulset podManagementPolicy ordered parallel
u: (<< statefulset [podManagementPolicy management gestion ordre order parallel parallele] >>)
	^keep() podManagementPolicy controls how Pods are created and deleted. OrderedReady (default) creates Pods sequentially (pod-0 must be Ready before pod-1 starts) and deletes in reverse order. Parallel launches or terminates all Pods simultaneously, useful when ordering is not required. \n
	podManagementPolicy controle comment les Pods sont crees et supprimes. OrderedReady (defaut) cree les Pods sequentiellement et les supprime dans l'ordre inverse. Parallel lance ou termine tous les Pods simultanement, utile quand l'ordre n'est pas necessaire.

#! statefulset serviceName headless service
u: (<< statefulset [serviceName service headless] >>)
	^keep() The serviceName field is required and must reference an existing Headless Service (clusterIP: None). This Service provides the network identity for each Pod: <pod-name>.<service-name>.<namespace>.svc.cluster.local. The Headless Service must be created before the StatefulSet. \n
	Le champ serviceName est obligatoire et doit referencer un Service Headless existant (clusterIP: None). Ce Service fournit l'identite reseau de chaque Pod. Le Service Headless doit etre cree avant le StatefulSet.

#! statefulset updateStrategy rolling ondelete
u: (<< statefulset [update mise_a_jour] [strategy strategie] >>)
	^keep() StatefulSet supports two updateStrategy types: RollingUpdate (default) updates Pods in reverse ordinal order (highest first). Use 'partition' to stage a canary rollout by only updating Pods with ordinal >= partition value. OnDelete requires manual Pod deletion to trigger updates. \n
	Le StatefulSet supporte deux types d'updateStrategy : RollingUpdate (defaut) met a jour les Pods dans l'ordre inverse. Utilisez 'partition' pour un deploiement canary. OnDelete necessite la suppression manuelle des Pods pour declencher les mises a jour.

#! statefulset vs deployment difference
u: (<< statefulset [vs versus contre ou] deployment >>)
	^keep() Use a Deployment for stateless apps where Pods are interchangeable. Use a StatefulSet when you need: stable unique Pod names, stable persistent storage per Pod, ordered rollout and scaling, or stable network identities. StatefulSets are more complex to operate and should only be used when their guarantees are needed. \n
	Utilisez un Deployment pour les apps stateless ou les Pods sont interchangeables. Utilisez un StatefulSet quand vous avez besoin de : noms stables, stockage persistant par Pod, deploiement ordonne ou identites reseau stables. Les StatefulSets sont plus complexes et ne devraient etre utilises que lorsque leurs garanties sont necessaires.

# ===========================================================================
# DAEMONSET
# ===========================================================================

topic: ~daemonset (daemonset daemonsets daemon agent noeud noeuds)

#! what is a daemonset
u: (<< [what que "qu est-ce"] [is est] [a un une] daemonset >>)
	^keep() A DaemonSet ensures that a copy of a Pod runs on all (or selected) nodes in the cluster. When a new node is added, the DaemonSet automatically schedules a Pod on it. When a node is removed, the Pod is garbage collected. \n
	Un DaemonSet garantit qu'une copie d'un Pod tourne sur tous (ou certains) noeuds du cluster. Quand un nouveau noeud est ajoute, le DaemonSet planifie automatiquement un Pod dessus. Quand un noeud est supprime, le Pod est collecte.

#! daemonset use cases examples
u: (<< daemonset [use usage cas utilisation example exemple] >>)
	^keep() Common DaemonSet use cases include: log collectors (Fluentd, Filebeat, Promtail), monitoring agents (Prometheus Node Exporter, Datadog agent), network plugins (Calico, Cilium, Weave), storage daemons (Ceph, GlusterFS), and security agents (Falco, Twistlock). \n
	Cas d'usage courants des DaemonSets : collecteurs de logs (Fluentd, Filebeat, Promtail), agents de monitoring (Node Exporter, Datadog), plugins reseau (Calico, Cilium), demons de stockage (Ceph, GlusterFS) et agents de securite (Falco, Twistlock).

#! daemonset updateStrategy rolling ondelete
u: (<< daemonset [update mise_a_jour] [strategy strategie] >>)
	^keep() DaemonSet supports two updateStrategy types: RollingUpdate (default) updates Pods one node at a time. Configure maxUnavailable (default 1) and maxSurge to control the pace. OnDelete only updates Pods when you manually delete them, giving you full control over the rollout. \n
	Le DaemonSet supporte deux types d'updateStrategy : RollingUpdate (defaut) met a jour les Pods un noeud a la fois. Configurez maxUnavailable et maxSurge. OnDelete ne met a jour les Pods que quand vous les supprimez manuellement.

#! daemonset node selection affinity
u: (<< daemonset [node noeud] [select selection affinity affinite] >>)
	^keep() To restrict a DaemonSet to specific nodes, use nodeSelector for simple label matching or nodeAffinity for advanced rules. You can also combine tolerations with taints to target dedicated node pools (e.g. GPU nodes, edge nodes). Without selectors, the DaemonSet runs on all schedulable nodes. \n
	Pour restreindre un DaemonSet a certains noeuds, utilisez nodeSelector pour un filtrage simple ou nodeAffinity pour des regles avancees. Combinez les tolerations avec les taints pour cibler des pools de noeuds dedies. Sans selecteurs, le DaemonSet tourne sur tous les noeuds planifiables.

# ===========================================================================
# REPLICASET
# ===========================================================================

topic: ~replicaset (replicaset replicasets "replica set")

#! what is a replicaset
u: (<< [what que "qu est-ce"] [is est] [a un une] [replicaset "replica set"] >>)
	^keep() A ReplicaSet ensures that a specified number of Pod replicas are running at any given time. It uses a label selector to identify and manage its Pods. If a Pod is deleted or fails, the ReplicaSet creates a replacement to maintain the desired count. \n
	Un ReplicaSet garantit qu'un nombre specifie de replicas de Pod tournent a tout moment. Il utilise un selecteur de labels pour identifier et gerer ses Pods. Si un Pod est supprime ou echoue, le ReplicaSet en cree un nouveau.

#! replicaset vs deployment
u: (<< [replicaset "replica set"] [vs versus contre ou difference] deployment >>)
	^keep() A ReplicaSet only ensures the desired number of Pods are running. A Deployment wraps a ReplicaSet and adds rolling updates, rollback capabilities, update history, and declarative update strategies. Always prefer Deployments over bare ReplicaSets. \n
	Un ReplicaSet garantit uniquement le nombre de Pods souhaite. Un Deployment encapsule un ReplicaSet et ajoute les mises a jour progressives, les rollbacks, l'historique et les strategies de mise a jour. Preferez toujours les Deployments aux ReplicaSets nus.

#! when to use replicaset directly
u: (<< [when quand] [use utiliser] [replicaset "replica set"] [directly directement] >>)
	^keep() You rarely need to create ReplicaSets directly. The main use case is custom update orchestration where you need programmatic control over Pod replacement (e.g. a custom controller). For standard workloads, always use Deployments. Some operators also create ReplicaSets internally. \n
	Vous avez rarement besoin de creer des ReplicaSets directement. Le principal cas d'usage est l'orchestration personnalisee des mises a jour avec un controle programmatique. Pour les charges de travail standard, utilisez toujours des Deployments.

# ===========================================================================
# JOB
# ===========================================================================

topic: ~k8s_job (job jobs batch tache taches completions parallelism)

#! what is a job
u: (<< [what que "qu est-ce"] [is est] [a un une] job >>)
	^keep() A Job creates one or more Pods and ensures that a specified number of them successfully terminate. Jobs are used for batch processing, one-off tasks, data migrations, and any finite workload. The Pod's restartPolicy must be OnFailure or Never. \n
	Un Job cree un ou plusieurs Pods et garantit qu'un nombre specifie d'entre eux se terminent avec succes. Les Jobs sont utilises pour le traitement par lots, les taches ponctuelles, les migrations de donnees et toute charge de travail finie. Le restartPolicy du Pod doit etre OnFailure ou Never.

#! job completions parallelism
u: (<< job [completions parallelism completion parallele parallelisme] >>)
	^keep() 'completions' sets the total number of successful Pod completions required (default 1). 'parallelism' sets the maximum number of Pods running concurrently (default 1). For example, completions=10 with parallelism=3 runs up to 3 Pods at a time until 10 have succeeded. Use completionMode 'Indexed' for indexed Jobs where each Pod gets a unique index. \n
	'completions' definit le nombre total de Pods devant reussir (defaut 1). 'parallelism' definit le nombre max de Pods simultanes (defaut 1). Par exemple, completions=10 avec parallelism=3 execute jusqu'a 3 Pods simultanement jusqu'a 10 succes.

#! job backoffLimit activeDeadlineSeconds
u: (<< job [backoff backoffLimit deadline activeDeadlineSeconds limite delai] >>)
	^keep() backoffLimit specifies the number of retries before marking the Job as failed (default 6). Uses exponential back-off (10s, 20s, 40s, ...). activeDeadlineSeconds sets the maximum duration for the entire Job; once reached, all running Pods are terminated and the Job is marked Failed. \n
	backoffLimit specifie le nombre de tentatives avant d'echouer le Job (defaut 6). Utilise un back-off exponentiel. activeDeadlineSeconds definit la duree maximale du Job ; une fois atteinte, tous les Pods sont arretes et le Job est marque Failed.

#! job ttlSecondsAfterFinished
u: (<< job [ttl ttlSecondsAfterFinished cleanup nettoyage] >>)
	^keep() ttlSecondsAfterFinished specifies the number of seconds after a Job finishes (Complete or Failed) before it is automatically deleted by the TTL controller. This cleans up finished Jobs and their Pods. Setting it to 0 deletes the Job immediately after completion. If unset, the Job is not automatically cleaned up. \n
	ttlSecondsAfterFinished specifie le nombre de secondes apres la fin d'un Job avant sa suppression automatique par le controleur TTL. Le mettre a 0 supprime le Job immediatement. Si non defini, le Job n'est pas automatiquement nettoye.

#! job podFailurePolicy
u: (<< job [podFailurePolicy failure echec politique] >>)
	^keep() podFailurePolicy (stable since Kubernetes 1.31) lets you define rules for handling Pod failures based on container exit codes and Pod conditions. You can specify actions like FailJob (fail immediately), Ignore (do not count toward backoffLimit), or Count (default). This enables fine-grained failure handling, e.g. failing fast on non-retryable errors. \n
	podFailurePolicy (stable depuis Kubernetes 1.31) permet de definir des regles de gestion des echecs de Pods basees sur les codes de sortie et conditions. Les actions possibles sont FailJob (echouer immediatement), Ignore (ne pas compter) ou Count (defaut). Cela permet une gestion fine des echecs.

# ===========================================================================
# CRONJOB
# ===========================================================================

topic: ~cronjob (cronjob cronjobs cron planifie planification schedule)

#! what is a cronjob
u: (<< [what que "qu est-ce"] [is est] [a un une] cronjob >>)
	^keep() A CronJob creates Jobs on a recurring schedule. It is the Kubernetes equivalent of a Unix cron task. Use CronJobs for periodic tasks like backups, report generation, log rotation, email sending, and database cleanup. The CronJob controller creates a new Job object at each scheduled time. \n
	Un CronJob cree des Jobs selon un planning recurrent. C'est l'equivalent Kubernetes d'une tache cron Unix. Utilisez les CronJobs pour des taches periodiques comme les sauvegardes, la generation de rapports, la rotation des logs et le nettoyage de base de donnees.

#! cronjob schedule field cron syntax
u: (<< cronjob [schedule planning planification cron syntaxe syntax] >>)
	^keep() The schedule field uses standard cron syntax with five fields: minute (0-59), hour (0-23), day-of-month (1-31), month (1-12), day-of-week (0-6). Examples: '*/5 * * * *' (every 5 min), '0 2 * * *' (daily at 2am), '0 0 * * 0' (weekly Sunday midnight). Also supports \@yearly, \@monthly, \@weekly, \@daily, and \@hourly macros. \n
	Le champ schedule utilise la syntaxe cron standard avec cinq champs : minute, heure, jour du mois, mois, jour de la semaine. Exemples : '*/5 * * * *' (toutes les 5 min), '0 2 * * *' (chaque jour a 2h). Supporte aussi les macros \@yearly, \@monthly, \@weekly, \@daily et \@hourly.

#! cronjob concurrencyPolicy allow forbid replace
u: (<< cronjob [concurrency concurrencyPolicy concurrence] >>)
	^keep() concurrencyPolicy controls what happens when a new Job is scheduled while a previous one is still running. Allow (default) permits concurrent Jobs. Forbid skips the new Job if the previous one is still running. Replace cancels the currently running Job and starts a new one. \n
	concurrencyPolicy controle ce qui se passe quand un nouveau Job est planifie alors que le precedent tourne encore. Allow (defaut) autorise les Jobs simultanes. Forbid saute le nouveau Job. Replace annule le Job en cours et en demarre un nouveau.

#! cronjob timeZone
u: (<< cronjob [timezone timeZone fuseau horaire zone temps] >>)
	^keep() The timeZone field, stable since Kubernetes 1.27, lets you specify the time zone for the schedule using standard IANA names (e.g. 'Europe/Paris', 'America/New_York', 'UTC'). Without it, the schedule is interpreted relative to the kube-controller-manager's time zone. Always set timeZone explicitly to avoid ambiguity across clusters. \n
	Le champ timeZone (stable depuis Kubernetes 1.27) permet de specifier le fuseau horaire avec les noms IANA (ex: 'Europe/Paris', 'America/New_York', 'UTC'). Sans lui, le planning est relatif au fuseau du kube-controller-manager. Definissez toujours timeZone explicitement pour eviter les ambiguites.
